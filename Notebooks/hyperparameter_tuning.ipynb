{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bd180cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Log fayl manzili\n",
    "log_path = r\"C:\\Users\\Rasulbek907\\Desktop\\Project_MP\\Log\\tuning.log\"\n",
    "\n",
    "# Log sozlamalari\n",
    "logging.basicConfig(\n",
    "    filename=log_path,\n",
    "    filemode='a',  # Append mode\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    level=logging.INFO\n",
    ")\n",
    "\n",
    "try:\n",
    "    logging.info(\"CSV fayl o'qilmoqda:...\")\n",
    "    df = pd.read_csv(r\"C:\\Users\\Rasulbek907\\Desktop\\Project_MP\\Data\\Feature_Selection\\Filtered_Features.csv\")\n",
    "    logging.info(f\"Fayl muvaffaqiyatli o'qildi. Satƒ±rlar soni: {len(df)} ustunlar soni: {len(df.columns)}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"CSV faylni o'qishda xatolik: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cbb85bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4993 entries, 0 to 4992\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0.1           4993 non-null   int64  \n",
      " 1   Unnamed: 0             4993 non-null   int64  \n",
      " 2   name_length            4993 non-null   float64\n",
      " 3   is_organic             4993 non-null   float64\n",
      " 4   quantity_value         4993 non-null   float64\n",
      " 5   quantity_unit_encoded  4993 non-null   float64\n",
      " 6   category_depth         4993 non-null   float64\n",
      " 7   country_count          4993 non-null   float64\n",
      " 8   product_age_days       4993 non-null   float64\n",
      " 9   created_month          4993 non-null   float64\n",
      " 10  main_category_encoded  4993 non-null   float64\n",
      " 11  nova_group             4993 non-null   float64\n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 468.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934f163d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python executable: c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Scripts\\python.exe\n",
      "Source path added: c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\Source\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Source papkasini yo'lga qo'shamiz\n",
    "source_path = os.path.abspath(\"../Source\")\n",
    "if source_path not in sys.path:\n",
    "    sys.path.append(source_path)\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "print(\"Source path added:\", source_path)\n",
    "\n",
    "# Endi modulni import qilamiz\n",
    "from preprosessing import Cleaner, Encoder, Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f70b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = Cleaner(df)\n",
    "df = cleaner.tozala().get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f07fdf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4993 entries, 0 to 4992\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0.1           4993 non-null   int64  \n",
      " 1   Unnamed: 0             4993 non-null   int64  \n",
      " 2   name_length            4993 non-null   float64\n",
      " 3   is_organic             4993 non-null   float64\n",
      " 4   quantity_value         4993 non-null   float64\n",
      " 5   quantity_unit_encoded  4993 non-null   float64\n",
      " 6   category_depth         4993 non-null   float64\n",
      " 7   country_count          4993 non-null   float64\n",
      " 8   product_age_days       4993 non-null   float64\n",
      " 9   created_month          4993 non-null   float64\n",
      " 10  main_category_encoded  4993 non-null   float64\n",
      " 11  nova_group             4993 non-null   float64\n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 468.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bbc3b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(df)\n",
    "df = encoder.encodla().get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1253441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4993 entries, 0 to 4992\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0.1           4993 non-null   int64  \n",
      " 1   Unnamed: 0             4993 non-null   int64  \n",
      " 2   name_length            4993 non-null   float64\n",
      " 3   is_organic             4993 non-null   float64\n",
      " 4   quantity_value         4993 non-null   float64\n",
      " 5   quantity_unit_encoded  4993 non-null   float64\n",
      " 6   category_depth         4993 non-null   float64\n",
      " 7   country_count          4993 non-null   float64\n",
      " 8   product_age_days       4993 non-null   float64\n",
      " 9   created_month          4993 non-null   float64\n",
      " 10  main_category_encoded  4993 non-null   float64\n",
      " 11  nova_group             4993 non-null   float64\n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 468.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d773e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler(df, target_col=\"nova_group\")\n",
    "scaled_df = scaler.scaling_qil().get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5d5c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4993 entries, 0 to 4992\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   Unnamed: 0.1           4993 non-null   int64  \n",
      " 1   Unnamed: 0             4993 non-null   int64  \n",
      " 2   name_length            4993 non-null   float64\n",
      " 3   is_organic             4993 non-null   float64\n",
      " 4   quantity_value         4993 non-null   float64\n",
      " 5   quantity_unit_encoded  4993 non-null   float64\n",
      " 6   category_depth         4993 non-null   float64\n",
      " 7   country_count          4993 non-null   float64\n",
      " 8   product_age_days       4993 non-null   float64\n",
      " 9   created_month          4993 non-null   float64\n",
      " 10  main_category_encoded  4993 non-null   float64\n",
      " 11  nova_group             4993 non-null   float64\n",
      "dtypes: float64(10), int64(2)\n",
      "memory usage: 468.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d21edb",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb846721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Logistic Regression natijalari:\n",
      "Accuracy: 0.5425425425425425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 500 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=500).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X va y ajratish\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# Train / Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Model yaratish\n",
    "log_model = LogisticRegression(max_iter=500, random_state=42)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred = log_model.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "print(\"üìä Logistic Regression natijalari:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3b151",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c39072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ Decision Tree Classifier natijalari:\n",
      "Accuracy: 0.46846846846846846\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# X va y ajratish\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# Train / Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Model yaratish (default sozlamalar bilan)\n",
    "dt_model = DecisionTreeClassifier()\n",
    "\n",
    "# O‚Äòqitish\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Natijalar\n",
    "print(\"üå≥ Decision Tree Classifier natijalari:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a1a88",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857c405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5825825825825826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "rf_model = RandomForestClassifier()  # default sozlamalar\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182921f7",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af3574b",
   "metadata": {},
   "source": [
    "# MANUAL SEARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d354303",
   "metadata": {},
   "source": [
    "# Description: Adjust hyperparameters by intuition or trial-and-error. # Pros: Simple # Cons: Inefficient, may miss optimal values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829f1f71",
   "metadata": {},
   "source": [
    "#  RandomForestClassifier + Manual Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3381922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ Random Forest (Manual Search) Accuracy: 0.5455455455455456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# üß© Model (qo‚Äòlda hyperparameterlar bilan)\n",
    "rf_manual = RandomForestClassifier(\n",
    "    n_estimators=2,    # daraxtlar soni\n",
    "    max_depth=5,       # maksimal chuqurlik\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# O‚Äòqitish\n",
    "rf_manual.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred_manual = rf_manual.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "r2_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(\"üå≤ Random Forest (Manual Search) Accuracy:\", r2_manual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ff5bac",
   "metadata": {},
   "source": [
    "#  DecisionTreeClassifier + Manual Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a4686e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≥ Decision Tree (Manual Search) Accuracy: 0.5455455455455456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# üå≥ Model ‚Äî qo‚Äòlda tanlangan hyperparametrlar bilan\n",
    "dt_manual = DecisionTreeClassifier(\n",
    "    criterion=\"entropy\",   # yoki \"gini\"\n",
    "    max_depth=5,           # maksimal chuqurlik\n",
    "    min_samples_split=4,   # bo‚Äòlinish uchun minimal namunalar\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# O‚Äòqitish\n",
    "dt_manual.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred_manual = dt_manual.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(\"üå≥ Decision Tree (Manual Search) Accuracy:\", acc_manual)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc17f43",
   "metadata": {},
   "source": [
    "#  LogisticRegression + Manual Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0924cf91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Logistic Regression (Manual Search) Accuracy: 0.5325325325325325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "log_manual = LogisticRegression(\n",
    "    solver=\"liblinear\",   # kichik datasetlar uchun yaxshi\n",
    "    penalty=\"l2\",         # regularizatsiya turi (\"l1\" yoki \"l2\")\n",
    "    C=0.7,                # regularizatsiya kuchi (kichik bo‚Äòlsa ‚Äî ko‚Äòproq jazo)\n",
    "    max_iter=500,         # iteratsiyalar soni\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Modelni o‚Äòqitish\n",
    "log_manual.fit(X_train, y_train)\n",
    "\n",
    "# Bashorat\n",
    "y_pred_manual = log_manual.predict(X_test)\n",
    "\n",
    "# Baholash\n",
    "acc_manual = accuracy_score(y_test, y_pred_manual)\n",
    "print(\"üìä Logistic Regression (Manual Search) Accuracy:\", acc_manual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7410908e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rasulbek907\\AppData\\Local\\Temp\\ipykernel_13996\\2719313647.py:38: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.\n",
      "  .applymap(color_change, subset=['O‚Äòzgarish'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_25d89_row0_col2, #T_25d89_row5_col2 {\n",
       "  color: black;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_25d89_row1_col2, #T_25d89_row3_col2 {\n",
       "  color: red;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_25d89_row2_col2, #T_25d89_row4_col2 {\n",
       "  color: green;\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_25d89\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_25d89_level0_col0\" class=\"col_heading level0 col0\" >Model nomi</th>\n",
       "      <th id=\"T_25d89_level0_col1\" class=\"col_heading level0 col1\" >Aniqlik (Accuracy)</th>\n",
       "      <th id=\"T_25d89_level0_col2\" class=\"col_heading level0 col2\" >O‚Äòzgarish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_25d89_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_25d89_row0_col0\" class=\"data row0 col0\" >üìä Logistic Regression</td>\n",
       "      <td id=\"T_25d89_row0_col1\" class=\"data row0 col1\" >0.543</td>\n",
       "      <td id=\"T_25d89_row0_col2\" class=\"data row0 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25d89_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_25d89_row1_col0\" class=\"data row1 col0\" >üå≥ Decision Tree Classifier</td>\n",
       "      <td id=\"T_25d89_row1_col1\" class=\"data row1 col1\" >0.468</td>\n",
       "      <td id=\"T_25d89_row1_col2\" class=\"data row1 col2\" >-0.074074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25d89_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_25d89_row2_col0\" class=\"data row2 col0\" >üå≤ Random Forest Classifier</td>\n",
       "      <td id=\"T_25d89_row2_col1\" class=\"data row2 col1\" >0.583</td>\n",
       "      <td id=\"T_25d89_row2_col2\" class=\"data row2 col2\" >0.114114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25d89_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_25d89_row3_col0\" class=\"data row3 col0\" >üìä Logistic Regression (Manual Search)</td>\n",
       "      <td id=\"T_25d89_row3_col1\" class=\"data row3 col1\" >0.533</td>\n",
       "      <td id=\"T_25d89_row3_col2\" class=\"data row3 col2\" >-0.050050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25d89_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_25d89_row4_col0\" class=\"data row4 col0\" >üå≥ Decision Tree (Manual Search)</td>\n",
       "      <td id=\"T_25d89_row4_col1\" class=\"data row4 col1\" >0.546</td>\n",
       "      <td id=\"T_25d89_row4_col2\" class=\"data row4 col2\" >0.013013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25d89_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_25d89_row5_col0\" class=\"data row5 col0\" >üå≤ Random Forest (Manual Search)</td>\n",
       "      <td id=\"T_25d89_row5_col1\" class=\"data row5 col1\" >0.546</td>\n",
       "      <td id=\"T_25d89_row5_col2\" class=\"data row5 col2\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d8a6d130e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# üìä Modellar natijalari\n",
    "results = pd.DataFrame({\n",
    "    'Model nomi': [\n",
    "        'üìä Logistic Regression',\n",
    "        'üå≥ Decision Tree Classifier',\n",
    "        'üå≤ Random Forest Classifier',\n",
    "        'üìä Logistic Regression (Manual Search)',\n",
    "        'üå≥ Decision Tree (Manual Search)',\n",
    "        'üå≤ Random Forest (Manual Search)'\n",
    "    ],\n",
    "    'Aniqlik (Accuracy)': [\n",
    "        0.5425425425425425,\n",
    "        0.46846846846846846,\n",
    "        0.5825825825825826,\n",
    "        0.5325325325325325,\n",
    "        0.5455455455455456,\n",
    "        0.5455455455455456\n",
    "    ]\n",
    "})\n",
    "\n",
    "# üîÑ O'sish yoki kamayishni aniqlash uchun avvalgi qator bilan farqni topamiz\n",
    "results['O‚Äòzgarish'] = results['Aniqlik (Accuracy)'].diff().fillna(0)\n",
    "\n",
    "# üé® Rang beruvchi funksiya\n",
    "def color_change(val):\n",
    "    if val > 0:\n",
    "        color = 'green'\n",
    "    elif val < 0:\n",
    "        color = 'red'\n",
    "    else:\n",
    "        color = 'black'\n",
    "    return f'color: {color}; font-weight: bold'\n",
    "\n",
    "# üßæ Natijani chiroyli chiqarish\n",
    "styled = results.style.format({'Aniqlik (Accuracy)': '{:.3f}'}) \\\n",
    "                      .applymap(color_change, subset=['O‚Äòzgarish'])\n",
    "\n",
    "styled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de833907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffcd5c6b",
   "metadata": {},
   "source": [
    "# GRID SEARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397e464a",
   "metadata": {},
   "source": [
    "# RandomForestClassifier + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "765f4e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "‚úÖ Eng yaxshi parametrlar:\n",
      "{'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "\n",
      "üìà Random Forest (Grid Search) Accuracy: 0.5755755755755756\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.42      0.16      0.23       114\n",
      "         2.0       0.75      0.07      0.13        42\n",
      "         3.0       0.53      0.14      0.22       220\n",
      "         4.0       0.58      0.93      0.71       525\n",
      "         5.0       0.68      0.37      0.48        98\n",
      "\n",
      "    accuracy                           0.58       999\n",
      "   macro avg       0.59      0.33      0.35       999\n",
      "weighted avg       0.57      0.58      0.50       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# üîç Grid Search uchun parametrlar to‚Äòplami\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],     # daraxtlar soni\n",
    "    'max_depth': [None, 5, 10, 15],     # daraxt chuqurligi\n",
    "    'min_samples_split': [2, 5, 10],    # bo‚Äòlinish uchun minimal namunalar\n",
    "    'min_samples_leaf': [1, 2, 4],      # barg tugunlari uchun minimal namunalar\n",
    "    'criterion': ['gini', 'entropy']    # ajratish mezoni\n",
    "}\n",
    "\n",
    "# üß† Grid Search sozlash\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                   # 5-fold cross validation\n",
    "    scoring='accuracy',     # aniqlikni baholash mezoni\n",
    "    n_jobs=-1,              # barcha yadroda bajarish\n",
    "    verbose=2               # jarayonni ko‚Äòrsatish\n",
    ")\n",
    "\n",
    "# üîß Modelni o‚Äòqitish (eng yaxshi parametrlarni topish)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi model va parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\nüìà Random Forest (Grid Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeda0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fac57b1",
   "metadata": {},
   "source": [
    "#  DecisionTreeClassifier + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f8539fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 270 candidates, totalling 1350 fits\n",
      "‚úÖ Eng yaxshi parametrlar:\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
      "\n",
      "üå≥ Decision Tree (Grid Search) Accuracy: 0.5455455455455456\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.32      0.13      0.19       114\n",
      "         2.0       1.00      0.05      0.09        42\n",
      "         3.0       0.44      0.10      0.17       220\n",
      "         4.0       0.56      0.91      0.69       525\n",
      "         5.0       0.62      0.29      0.39        98\n",
      "\n",
      "    accuracy                           0.55       999\n",
      "   macro avg       0.59      0.30      0.31       999\n",
      "weighted avg       0.53      0.55      0.46       999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# üîç Grid Search uchun parametrlar to‚Äòplami\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # bo‚Äòlinish mezoni\n",
    "    'max_depth': [None, 5, 10, 20, 30],             # daraxt chuqurligi\n",
    "    'min_samples_split': [2, 5, 10],                # tugunni bo‚Äòlish uchun minimal namunalar\n",
    "    'min_samples_leaf': [1, 2, 4],                  # barg tugun uchun eng kam namunalar\n",
    "    'splitter': ['best', 'random']                  # tugun ajratish usuli\n",
    "}\n",
    "\n",
    "# üß† Grid Search sozlash\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# üîß Modelni o‚Äòqitish\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_dt_model = grid_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\nüå≥ Decision Tree (Grid Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834aaeb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77b97406",
   "metadata": {},
   "source": [
    "#  Logistic Regression + Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fcdf310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 60 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "125 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 72, in _check_solver\n",
      "    raise ValueError(\n",
      "        f\"Only 'saga' solver supports elasticnet penalty, got solver={solver}.\"\n",
      "    )\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan 0.52629007 0.5255385  0.53380008 0.5310457  0.5257885\n",
      "        nan        nan        nan 0.54105821        nan 0.5257885\n",
      "        nan 0.53630258 0.5257885  0.54231072 0.53655352 0.5257885\n",
      "        nan        nan        nan 0.54105821        nan 0.5257885\n",
      "        nan 0.53855603 0.5257885  0.54180978 0.53630352 0.5257885\n",
      "        nan        nan        nan 0.54105821        nan 0.5257885\n",
      "        nan 0.53880634 0.5257885  0.54080884 0.53630352 0.5257885\n",
      "        nan        nan        nan 0.54105821        nan 0.5257885\n",
      "        nan 0.53880634 0.5257885  0.54055759 0.53630352 0.5257885\n",
      "        nan        nan        nan 0.54105821        nan 0.5257885 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Eng yaxshi parametrlar:\n",
      "{'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "\n",
      "üìà Logistic Regression (Grid Search) Accuracy: 0.5455455455455456\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       1.00      0.02      0.03       114\n",
      "         2.0       0.00      0.00      0.00        42\n",
      "         3.0       0.33      0.01      0.03       220\n",
      "         4.0       0.54      0.98      0.70       525\n",
      "         5.0       0.69      0.26      0.37        98\n",
      "\n",
      "    accuracy                           0.55       999\n",
      "   macro avg       0.51      0.25      0.23       999\n",
      "weighted avg       0.54      0.55      0.41       999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# üîç Grid Search uchun parametrlar to‚Äòplami\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],  # jarima turi\n",
    "    'C': [0.01, 0.1, 1, 10, 100],                 # regularizatsiya kuchi\n",
    "    'solver': ['lbfgs', 'liblinear', 'saga'],     # optimallashtirish algoritmi\n",
    "}\n",
    "\n",
    "# üß† Grid Search sozlash\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',     # aniqlik mezoni\n",
    "    n_jobs=-1,              # barcha CPU yadrolarida bajarish\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# üîß O‚Äòqitish\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_log_model = grid_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_log_model.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\nüìà Logistic Regression (Grid Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df69ffe7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f92a4338",
   "metadata": {},
   "source": [
    "| ‚Ññ | Model nomi                           | Eng yaxshi parametrlar                                                                                          | Aniqlik (Accuracy) |\n",
    "| - | ------------------------------------ | --------------------------------------------------------------------------------------------------------------- | ------------------ |\n",
    "| 1 | üå≤ Random Forest (Grid Search)       | {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150} | 0.576              |\n",
    "| 2 | üå≥ Decision Tree (Grid Search)       | {'criterion': 'entropy', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}     | 0.546              |\n",
    "| 3 | üìä Logistic Regression (Grid Search) | {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}                                                                  | 0.546              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb292ba1",
   "metadata": {},
   "source": [
    "# RANDOM SEARCH "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f92bdb",
   "metadata": {},
   "source": [
    "# RandomForestClasssifier + Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "305a27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "‚úÖ Eng yaxshi parametrlar:\n",
      "{'bootstrap': False, 'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 9, 'n_estimators': 180}\n",
      "\n",
      "üå≤ Random Forest (Random Search) Accuracy: 0.5687583444592791\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.46      0.18      0.26       170\n",
      "         2.0       0.80      0.06      0.12        64\n",
      "         3.0       0.47      0.16      0.24       330\n",
      "         4.0       0.58      0.91      0.71       787\n",
      "         5.0       0.58      0.31      0.40       147\n",
      "\n",
      "    accuracy                           0.57      1498\n",
      "   macro avg       0.58      0.32      0.35      1498\n",
      "weighted avg       0.55      0.57      0.50      1498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import randint\n",
    "\n",
    "# üîπ Ma‚Äôlumotni ajratish\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# üîπ Train/Test bo‚Äòlish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# üå≤ Model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# üéØ Random Search uchun parametrlar diapazoni\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 300),         # daraxtlar soni\n",
    "    'max_depth': [None, 5, 10, 20, 30],       # chuqurlik\n",
    "    'min_samples_split': randint(2, 10),      # bo‚Äòlinish uchun minimal namunalar\n",
    "    'min_samples_leaf': randint(1, 5),        # barg uchun minimal namunalar\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # ajratish mezoni\n",
    "    'bootstrap': [True, False]                # bootstrap namunalar ishlatilsinmi\n",
    "}\n",
    "\n",
    "# ‚öôÔ∏è Random Search sozlash\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,               # 20 ta tasodifiy kombinatsiyani sinaydi\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# üîß Modelni o‚Äòqitish\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\nüå≤ Random Forest (Random Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e9ace",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc95aae3",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier + Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06d89519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "‚úÖ Eng yaxshi parametrlar:\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 13, 'splitter': 'random'}\n",
      "\n",
      "üå≥ Decision Tree (Random Search) Accuracy: 0.5393858477970628\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.00      0.00      0.00       170\n",
      "         2.0       0.45      0.08      0.13        64\n",
      "         3.0       0.08      0.00      0.01       330\n",
      "         4.0       0.54      0.98      0.70       787\n",
      "         5.0       0.64      0.23      0.34       147\n",
      "\n",
      "    accuracy                           0.54      1498\n",
      "   macro avg       0.34      0.26      0.24      1498\n",
      "weighted avg       0.38      0.54      0.41      1498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# üîπ X va y ajratish\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# üîπ Train/Test bo‚Äòlish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# üå≥ Model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# üéØ Random Search uchun parametrlar diapazoni\n",
    "param_dist = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],   # ajratish mezoni\n",
    "    'splitter': ['best', 'random'],                 # bo‚Äòlinish strategiyasi\n",
    "    'max_depth': [None, 5, 10, 20, 30, 50],         # daraxt chuqurligi\n",
    "    'min_samples_split': randint(2, 20),            # tugunni bo‚Äòlish uchun minimal namunalar\n",
    "    'min_samples_leaf': randint(1, 10),             # barg uchun minimal namunalar\n",
    "    'max_features': [None, 'sqrt', 'log2']          # xususiyat tanlash strategiyasi\n",
    "}\n",
    "\n",
    "# ‚öôÔ∏è Random Search sozlash\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,               # 20 ta kombinatsiya sinovdan o‚Äòtadi\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# üîß Modelni o‚Äòqitish\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_dt_model = random_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_dt_model.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\nüå≥ Decision Tree (Random Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92333f2",
   "metadata": {},
   "source": [
    "# Logistic regression + Random Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02e952a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "‚úÖ Eng yaxshi parametrlar:\n",
      "{'C': np.float64(6.813075385877797), 'class_weight': None, 'fit_intercept': True, 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "\n",
      "‚öôÔ∏è Logistic Regression (Random Search) Accuracy: 0.5353805073431241\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.33      0.01      0.01       170\n",
      "         2.0       0.00      0.00      0.00        64\n",
      "         3.0       0.33      0.02      0.03       330\n",
      "         4.0       0.54      0.99      0.70       787\n",
      "         5.0       0.67      0.10      0.17       147\n",
      "\n",
      "    accuracy                           0.54      1498\n",
      "   macro avg       0.37      0.22      0.18      1498\n",
      "weighted avg       0.46      0.54      0.39      1498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:516: FitFailedWarning: \n",
      "50 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 72, in _check_solver\n",
      "    raise ValueError(\n",
      "        f\"Only 'saga' solver supports elasticnet penalty, got solver={solver}.\"\n",
      "    )\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1228, in fit\n",
      "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
      "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 77, in _check_solver\n",
      "    raise ValueError(\"penalty=None is not supported for the liblinear solver\")\n",
      "ValueError: penalty=None is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1365, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1218, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 64, in _check_solver\n",
      "    raise ValueError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1135: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.52560801 0.28927039 0.43633763        nan\n",
      " 0.52560801        nan 0.52560801 0.53648069        nan        nan\n",
      "        nan 0.28927039 0.28927039 0.28927039        nan        nan\n",
      " 0.27982833        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from scipy.stats import uniform\n",
    "\n",
    "# üîπ X va y ajratish\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# üîπ Train/Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ‚öôÔ∏è Model\n",
    "log_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# üéØ Random Search uchun parametrlar diapazoni\n",
    "param_dist = {\n",
    "    'solver': ['liblinear', 'lbfgs', 'saga'],  # optimallashtirish algoritmlari\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],  # regularizatsiya turi\n",
    "    'C': uniform(0.01, 10),                     # regularizatsiya kuchi\n",
    "    'fit_intercept': [True, False],             # interceptni o‚Äòrganish yoki yo‚Äòq\n",
    "    'class_weight': [None, 'balanced']          # sinflarni balanslash\n",
    "}\n",
    "\n",
    "# üß† Random Search sozlash\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=log_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,               # 20 ta kombinatsiya sinovdan o‚Äòtadi\n",
    "    cv=5,                    # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# üîß Modelni o‚Äòqitish\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(random_search.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_log_model = random_search.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_log_model.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\n‚öôÔ∏è Logistic Regression (Random Search) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca17321",
   "metadata": {},
   "source": [
    "| ‚Ññ | Model nomi                             | Eng yaxshi parametrlar                                                                                                               | Accuracy |\n",
    "| - | -------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------ | -------- |\n",
    "| 1 | üå≤ Random Forest (Random Search)       | {'bootstrap': False, 'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 9, 'n_estimators': 180}       | 0.5688   |\n",
    "| 2 | üå≥ Decision Tree (Random Search)       | {'criterion': 'entropy', 'max_depth': 5, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 13, 'splitter': 'random'} | 0.5394   |\n",
    "| 3 | üìä Logistic Regression (Random Search) | {'C': 6.813, 'class_weight': None, 'fit_intercept': True, 'penalty': 'l2', 'solver': 'liblinear'}                                    | 0.5354   |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b700de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0b8540a",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb558a",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b672c",
   "metadata": {},
   "source": [
    "# Bayesian Optimization +  RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b203dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "‚úÖ Eng yaxshi parametrlar:\n",
      "OrderedDict({'bootstrap': True, 'max_depth': 16, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 300})\n",
      "\n",
      "üå≤ Random Forest (Bayesian Optimization) Accuracy: 0.5781041388518025\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.41      0.16      0.23       170\n",
      "         2.0       0.71      0.16      0.26        64\n",
      "         3.0       0.51      0.18      0.27       330\n",
      "         4.0       0.59      0.91      0.72       787\n",
      "         5.0       0.61      0.36      0.45       147\n",
      "\n",
      "    accuracy                           0.58      1498\n",
      "   macro avg       0.57      0.35      0.39      1498\n",
      "weighted avg       0.56      0.58      0.52      1498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# üå≤ Model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# üéØ Bayesian Optimization uchun parametrlar oralig‚Äòi\n",
    "param_space = {\n",
    "    'n_estimators': (50, 300),           # daraxtlar soni\n",
    "    'max_depth': (3, 30),                # chuqurlik\n",
    "    'min_samples_split': (2, 10),        # tugunni bo‚Äòlish uchun minimal namunalar\n",
    "    'min_samples_leaf': (1, 5),          # barg uchun minimal namunalar\n",
    "    'max_features': ['sqrt', 'log2', None], # xususiyat tanlash usuli\n",
    "    'bootstrap': [True, False]           # bootstrap namunalar ishlatilsinmi\n",
    "}\n",
    "\n",
    "# ‚öôÔ∏è BayesSearchCV sozlash\n",
    "opt = BayesSearchCV(\n",
    "    estimator=rf,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=25,              # necha kombinatsiya sinovdan o‚Äòtadi\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# üîß Modelni o‚Äòqitish\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(opt.best_params_)\n",
    "\n",
    "# Eng yaxshi model\n",
    "best_rf = opt.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\nüå≤ Random Forest (Bayesian Optimization) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccda6821",
   "metadata": {},
   "source": [
    "# Bayesian Optimization +  DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e868fc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "‚úÖ Eng yaxshi parametrlar:\n",
      "OrderedDict({'criterion': 'entropy', 'max_depth': 5, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 11})\n",
      "\n",
      "üåø Decision Tree (Bayesian Optimization) Accuracy: 0.5440587449933244\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.45      0.03      0.06       170\n",
      "         2.0       0.40      0.03      0.06        64\n",
      "         3.0       0.49      0.08      0.14       330\n",
      "         4.0       0.55      0.95      0.69       787\n",
      "         5.0       0.57      0.24      0.34       147\n",
      "\n",
      "    accuracy                           0.54      1498\n",
      "   macro avg       0.49      0.27      0.26      1498\n",
      "weighted avg       0.52      0.54      0.44      1498\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# üîπ X va y ajratish\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# üîπ Train/Test ajratish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# üå≥ Model yaratish\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# üéØ Bayesian Optimization uchun parametrlar oralig‚Äòi\n",
    "param_space = {\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'],  # qaror mezoni\n",
    "    'max_depth': (2, 30),                          # maksimal chuqurlik\n",
    "    'min_samples_split': (2, 20),                  # bo‚Äòlinish uchun minimal namunalar\n",
    "    'min_samples_leaf': (1, 10),                   # bargdagi minimal namunalar\n",
    "    'max_features': ['sqrt', 'log2', None]         # xususiyat tanlash strategiyasi\n",
    "}\n",
    "\n",
    "# ‚öôÔ∏è BayesSearchCV sozlash\n",
    "opt = BayesSearchCV(\n",
    "    estimator=dt,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=25,              # sinovlar soni\n",
    "    cv=5,                   # 5-fold cross-validation\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# üîß Modelni o‚Äòqitish\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "# üèÜ Eng yaxshi parametrlar\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\")\n",
    "print(opt.best_params_)\n",
    "\n",
    "# Eng yaxshi modelni olish\n",
    "best_dt = opt.best_estimator_\n",
    "\n",
    "# Bashorat\n",
    "y_pred = best_dt.predict(X_test)\n",
    "\n",
    "# üìä Natijalar\n",
    "print(\"\\nüåø Decision Tree (Bayesian Optimization) Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90df8f6e",
   "metadata": {},
   "source": [
    "# Bayesian Optimization +  LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9e4d29f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "‚úÖ Eng yaxshi parametrlar: OrderedDict({'C': 0.285084511567709, 'penalty': 'l2', 'solver': 'lbfgs'})\n",
      "\n",
      "üìà Accuracy: 0.5393858477970628\n",
      "\n",
      "üîç To‚Äòliq tahlil:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.40      0.01      0.02       170\n",
      "         2.0       0.00      0.00      0.00        64\n",
      "         3.0       0.33      0.02      0.03       330\n",
      "         4.0       0.54      0.98      0.70       787\n",
      "         5.0       0.62      0.20      0.30       147\n",
      "\n",
      "    accuracy                           0.54      1498\n",
      "   macro avg       0.38      0.24      0.21      1498\n",
      "weighted avg       0.46      0.54      0.40      1498\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "# üîπ X va y\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# ‚öôÔ∏è Model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# üîß Parametr oralig‚Äòi (mos juftliklar bilan)\n",
    "param_space = [\n",
    "    ({'solver': ['liblinear'], 'penalty': ['l1', 'l2'], 'C': (1e-4, 10.0, 'log-uniform')}),\n",
    "    ({'solver': ['lbfgs'], 'penalty': ['l2', None], 'C': (1e-4, 10.0, 'log-uniform')}),\n",
    "    ({'solver': ['saga'], 'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "      'C': (1e-4, 10.0, 'log-uniform'), 'l1_ratio': (0, 1.0)})\n",
    "]\n",
    "\n",
    "# üöÄ Bayesian Optimization\n",
    "opt = BayesSearchCV(\n",
    "    estimator=log_reg,\n",
    "    search_spaces=param_space,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Eng yaxshi parametrlar:\", opt.best_params_)\n",
    "best_model = opt.best_estimator_\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(\"\\nüìà Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nüîç To‚Äòliq tahlil:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11630d",
   "metadata": {},
   "source": [
    "| ‚Ññ | Model nomi                                     | Accuracy | Macro Avg (Precision) | Macro Avg (Recall) | Macro Avg (F1-score) | Weighted Avg (Precision) | Weighted Avg (Recall) | Weighted Avg (F1-score) | Support |\n",
    "| - | ---------------------------------------------- | -------- | --------------------- | ------------------ | -------------------- | ------------------------ | --------------------- | ----------------------- | ------- |\n",
    "| 1 | RandomForestClassifier (Bayesian Optimization) | 0.58     | 0.57                  | 0.35               | 0.39                 | 0.56                     | 0.58                  | 0.52                    | 1498    |\n",
    "| 2 | DecisionTreeClassifier (Bayesian Optimization) | 0.54     | 0.49                  | 0.27               | 0.26                 | 0.52                     | 0.54                  | 0.44                    | 1498    |\n",
    "| 3 | LogisticRegression (Bayesian Optimization)     | 0.54     | 0.38                  | 0.24               | 0.21                 | 0.46                     | 0.54                  | 0.40                    | 1498    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfabbe",
   "metadata": {},
   "source": [
    "# Optuna (Automated / Advanced Method) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55c0c8",
   "metadata": {},
   "source": [
    "# Optuna + Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cda050f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-04 10:14:58,261] A new study created in memory with name: no-name-78b6b4cb-601f-4037-a2af-9e8ff87de8f4\n",
      "[I 2025-11-04 10:14:59,264] Trial 0 finished with value: 0.5442060085836911 and parameters: {'n_estimators': 179, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 9, 'max_features': 'log2'}. Best is trial 0 with value: 0.5442060085836911.\n",
      "[I 2025-11-04 10:14:59,646] Trial 1 finished with value: 0.5413447782546496 and parameters: {'n_estimators': 58, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 0 with value: 0.5442060085836911.\n",
      "[I 2025-11-04 10:15:01,020] Trial 2 finished with value: 0.5444921316165952 and parameters: {'n_estimators': 214, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 'log2'}. Best is trial 2 with value: 0.5444921316165952.\n",
      "[I 2025-11-04 10:15:02,661] Trial 3 finished with value: 0.5490701001430615 and parameters: {'n_estimators': 251, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 3 with value: 0.5490701001430615.\n",
      "[I 2025-11-04 10:15:03,429] Trial 4 finished with value: 0.5344778254649499 and parameters: {'n_estimators': 128, 'max_depth': 3, 'min_samples_split': 9, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 3 with value: 0.5490701001430615.\n",
      "[I 2025-11-04 10:15:05,208] Trial 5 finished with value: 0.5496423462088698 and parameters: {'n_estimators': 290, 'max_depth': 12, 'min_samples_split': 9, 'min_samples_leaf': 7, 'max_features': 'log2'}. Best is trial 5 with value: 0.5496423462088698.\n",
      "[I 2025-11-04 10:15:05,833] Trial 6 finished with value: 0.545922746781116 and parameters: {'n_estimators': 54, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 5 with value: 0.5496423462088698.\n",
      "[I 2025-11-04 10:15:06,977] Trial 7 finished with value: 0.5496423462088699 and parameters: {'n_estimators': 176, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 7 with value: 0.5496423462088699.\n",
      "[I 2025-11-04 10:15:08,308] Trial 8 finished with value: 0.5344778254649499 and parameters: {'n_estimators': 216, 'max_depth': 3, 'min_samples_split': 10, 'min_samples_leaf': 10, 'max_features': 'log2'}. Best is trial 7 with value: 0.5496423462088699.\n",
      "[I 2025-11-04 10:15:09,076] Trial 9 finished with value: 0.5456366237482118 and parameters: {'n_estimators': 106, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 8, 'max_features': 'sqrt'}. Best is trial 7 with value: 0.5496423462088699.\n",
      "[I 2025-11-04 10:15:10,197] Trial 10 finished with value: 0.5539341917024321 and parameters: {'n_estimators': 144, 'max_depth': 27, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.5539341917024321.\n",
      "[I 2025-11-04 10:15:11,364] Trial 11 finished with value: 0.5533619456366238 and parameters: {'n_estimators': 140, 'max_depth': 29, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.5539341917024321.\n",
      "[I 2025-11-04 10:15:12,478] Trial 12 finished with value: 0.5539341917024321 and parameters: {'n_estimators': 119, 'max_depth': 29, 'min_samples_split': 4, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.5539341917024321.\n",
      "[I 2025-11-04 10:15:13,291] Trial 13 finished with value: 0.5539341917024321 and parameters: {'n_estimators': 91, 'max_depth': 18, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.5539341917024321.\n",
      "[I 2025-11-04 10:15:14,410] Trial 14 finished with value: 0.5525035765379113 and parameters: {'n_estimators': 147, 'max_depth': 30, 'min_samples_split': 4, 'min_samples_leaf': 3, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.5539341917024321.\n",
      "[I 2025-11-04 10:15:15,249] Trial 15 finished with value: 0.5536480686695279 and parameters: {'n_estimators': 90, 'max_depth': 20, 'min_samples_split': 4, 'min_samples_leaf': 2, 'max_features': 'sqrt'}. Best is trial 10 with value: 0.5539341917024321.\n",
      "[I 2025-11-04 10:15:17,101] Trial 16 finished with value: 0.5582260371959943 and parameters: {'n_estimators': 162, 'max_depth': 23, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 16 with value: 0.5582260371959943.\n",
      "[I 2025-11-04 10:15:19,069] Trial 17 finished with value: 0.555650929899857 and parameters: {'n_estimators': 198, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 16 with value: 0.5582260371959943.\n",
      "[I 2025-11-04 10:15:21,153] Trial 18 finished with value: 0.5545064377682403 and parameters: {'n_estimators': 208, 'max_depth': 21, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 16 with value: 0.5582260371959943.\n",
      "[I 2025-11-04 10:15:23,646] Trial 19 finished with value: 0.5530758226037196 and parameters: {'n_estimators': 270, 'max_depth': 14, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 16 with value: 0.5582260371959943.\n",
      "[I 2025-11-04 10:15:26,330] Trial 20 finished with value: 0.5587982832618026 and parameters: {'n_estimators': 236, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 20 with value: 0.5587982832618026.\n",
      "[I 2025-11-04 10:15:29,066] Trial 21 finished with value: 0.559656652360515 and parameters: {'n_estimators': 234, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:31,779] Trial 22 finished with value: 0.5567954220314736 and parameters: {'n_estimators': 248, 'max_depth': 24, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:34,345] Trial 23 finished with value: 0.559656652360515 and parameters: {'n_estimators': 237, 'max_depth': 16, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:37,056] Trial 24 finished with value: 0.5570815450643777 and parameters: {'n_estimators': 240, 'max_depth': 17, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:39,979] Trial 25 finished with value: 0.5547925608011446 and parameters: {'n_estimators': 293, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 6, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:42,446] Trial 26 finished with value: 0.559656652360515 and parameters: {'n_estimators': 232, 'max_depth': 12, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:45,180] Trial 27 finished with value: 0.5567954220314736 and parameters: {'n_estimators': 272, 'max_depth': 11, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:47,651] Trial 28 finished with value: 0.5565092989985693 and parameters: {'n_estimators': 231, 'max_depth': 19, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n",
      "[I 2025-11-04 10:15:49,639] Trial 29 finished with value: 0.5542203147353361 and parameters: {'n_estimators': 191, 'max_depth': 12, 'min_samples_split': 6, 'min_samples_leaf': 5, 'max_features': None}. Best is trial 21 with value: 0.559656652360515.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü•á Eng yaxshi parametrlar:\n",
      "{'n_estimators': 234, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None}\n",
      "‚úÖ Eng yaxshi aniqlik (CV): 0.5597\n",
      "üéØ Test aniqligi: 0.5821\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# üîπ X va y ajratish\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# üîπ Train/test bo‚Äòlish (stratify bilan)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# üîπ Optuna maqsad funksiyasi\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 300)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 30)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    max_features = trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\", None])\n",
    "\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=max_features,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # 3-fold cross-validation orqali aniqlikni o‚Äòlchash\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# üîπ Study yaratish\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# üîπ Natijalar\n",
    "print(\"ü•á Eng yaxshi parametrlar:\")\n",
    "print(study.best_params)\n",
    "print(f\"‚úÖ Eng yaxshi aniqlik (CV): {study.best_value:.4f}\")\n",
    "\n",
    "# üîπ Eng yaxshi modelni yaratish va testda sinovdan o‚Äòtkazish\n",
    "best_model = RandomForestClassifier(**study.best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"üéØ Test aniqligi: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e5bef4",
   "metadata": {},
   "source": [
    "# Optuna + Decision Tree  Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "51ddc59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-04 10:16:07,370] A new study created in memory with name: no-name-9811e2b0-b359-453b-aa2f-c2dc3532dc8d\n",
      "[I 2025-11-04 10:16:07,463] Trial 0 finished with value: 0.48440629470672386 and parameters: {'criterion': 'gini', 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 8, 'splitter': 'best'}. Best is trial 0 with value: 0.48440629470672386.\n",
      "[I 2025-11-04 10:16:07,555] Trial 1 finished with value: 0.4663805436337625 and parameters: {'criterion': 'entropy', 'max_depth': 17, 'min_samples_split': 5, 'min_samples_leaf': 9, 'splitter': 'best'}. Best is trial 0 with value: 0.48440629470672386.\n",
      "[I 2025-11-04 10:16:07,643] Trial 2 finished with value: 0.4666666666666666 and parameters: {'criterion': 'entropy', 'max_depth': 11, 'min_samples_split': 3, 'min_samples_leaf': 2, 'splitter': 'best'}. Best is trial 0 with value: 0.48440629470672386.\n",
      "[I 2025-11-04 10:16:07,704] Trial 3 finished with value: 0.46151645207439196 and parameters: {'criterion': 'gini', 'max_depth': 29, 'min_samples_split': 4, 'min_samples_leaf': 7, 'splitter': 'best'}. Best is trial 0 with value: 0.48440629470672386.\n",
      "[I 2025-11-04 10:16:07,729] Trial 4 finished with value: 0.4869814020028613 and parameters: {'criterion': 'log_loss', 'max_depth': 13, 'min_samples_split': 13, 'min_samples_leaf': 5, 'splitter': 'random'}. Best is trial 4 with value: 0.4869814020028613.\n",
      "[I 2025-11-04 10:16:07,830] Trial 5 finished with value: 0.43462088698140205 and parameters: {'criterion': 'entropy', 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 2, 'splitter': 'best'}. Best is trial 4 with value: 0.4869814020028613.\n",
      "[I 2025-11-04 10:16:07,854] Trial 6 finished with value: 0.4895565092989986 and parameters: {'criterion': 'entropy', 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 9, 'splitter': 'random'}. Best is trial 6 with value: 0.4895565092989986.\n",
      "[I 2025-11-04 10:16:07,877] Trial 7 finished with value: 0.4861230329041488 and parameters: {'criterion': 'gini', 'max_depth': 28, 'min_samples_split': 12, 'min_samples_leaf': 8, 'splitter': 'random'}. Best is trial 6 with value: 0.4895565092989986.\n",
      "[I 2025-11-04 10:16:07,938] Trial 8 finished with value: 0.4669527896995708 and parameters: {'criterion': 'gini', 'max_depth': 24, 'min_samples_split': 12, 'min_samples_leaf': 8, 'splitter': 'best'}. Best is trial 6 with value: 0.4895565092989986.\n",
      "[I 2025-11-04 10:16:07,994] Trial 9 finished with value: 0.47525035765379114 and parameters: {'criterion': 'gini', 'max_depth': 27, 'min_samples_split': 15, 'min_samples_leaf': 10, 'splitter': 'best'}. Best is trial 6 with value: 0.4895565092989986.\n",
      "[I 2025-11-04 10:16:08,019] Trial 10 finished with value: 0.5344778254649499 and parameters: {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_split': 19, 'min_samples_leaf': 5, 'splitter': 'random'}. Best is trial 10 with value: 0.5344778254649499.\n",
      "[I 2025-11-04 10:16:08,045] Trial 11 finished with value: 0.5339055793991416 and parameters: {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_split': 18, 'min_samples_leaf': 5, 'splitter': 'random'}. Best is trial 10 with value: 0.5344778254649499.\n",
      "[I 2025-11-04 10:16:08,070] Trial 12 finished with value: 0.5339055793991416 and parameters: {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_split': 20, 'min_samples_leaf': 5, 'splitter': 'random'}. Best is trial 10 with value: 0.5344778254649499.\n",
      "[I 2025-11-04 10:16:08,095] Trial 13 finished with value: 0.5339055793991416 and parameters: {'criterion': 'log_loss', 'max_depth': 2, 'min_samples_split': 20, 'min_samples_leaf': 4, 'splitter': 'random'}. Best is trial 10 with value: 0.5344778254649499.\n",
      "[I 2025-11-04 10:16:08,122] Trial 14 finished with value: 0.5359084406294706 and parameters: {'criterion': 'log_loss', 'max_depth': 6, 'min_samples_split': 17, 'min_samples_leaf': 3, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,150] Trial 15 finished with value: 0.5227467811158798 and parameters: {'criterion': 'log_loss', 'max_depth': 7, 'min_samples_split': 16, 'min_samples_leaf': 1, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,176] Trial 16 finished with value: 0.5244635193133048 and parameters: {'criterion': 'log_loss', 'max_depth': 7, 'min_samples_split': 17, 'min_samples_leaf': 3, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,206] Trial 17 finished with value: 0.5324749642346208 and parameters: {'criterion': 'log_loss', 'max_depth': 6, 'min_samples_split': 9, 'min_samples_leaf': 6, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,233] Trial 18 finished with value: 0.5359084406294706 and parameters: {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_split': 14, 'min_samples_leaf': 3, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,267] Trial 19 finished with value: 0.46781115879828333 and parameters: {'criterion': 'log_loss', 'max_depth': 22, 'min_samples_split': 14, 'min_samples_leaf': 3, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,297] Trial 20 finished with value: 0.48412017167381977 and parameters: {'criterion': 'log_loss', 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,323] Trial 21 finished with value: 0.534763948497854 and parameters: {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_split': 18, 'min_samples_leaf': 4, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,356] Trial 22 finished with value: 0.47525035765379114 and parameters: {'criterion': 'log_loss', 'max_depth': 14, 'min_samples_split': 17, 'min_samples_leaf': 3, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,387] Trial 23 finished with value: 0.5127324749642346 and parameters: {'criterion': 'log_loss', 'max_depth': 9, 'min_samples_split': 15, 'min_samples_leaf': 4, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,415] Trial 24 finished with value: 0.534763948497854 and parameters: {'criterion': 'log_loss', 'max_depth': 5, 'min_samples_split': 18, 'min_samples_leaf': 4, 'splitter': 'random'}. Best is trial 14 with value: 0.5359084406294706.\n",
      "[I 2025-11-04 10:16:08,440] Trial 25 finished with value: 0.5370529327610872 and parameters: {'criterion': 'log_loss', 'max_depth': 4, 'min_samples_split': 16, 'min_samples_leaf': 2, 'splitter': 'random'}. Best is trial 25 with value: 0.5370529327610872.\n",
      "[I 2025-11-04 10:16:08,473] Trial 26 finished with value: 0.5170243204577968 and parameters: {'criterion': 'log_loss', 'max_depth': 8, 'min_samples_split': 14, 'min_samples_leaf': 2, 'splitter': 'random'}. Best is trial 25 with value: 0.5370529327610872.\n",
      "[I 2025-11-04 10:16:08,507] Trial 27 finished with value: 0.5370529327610872 and parameters: {'criterion': 'log_loss', 'max_depth': 4, 'min_samples_split': 16, 'min_samples_leaf': 1, 'splitter': 'random'}. Best is trial 25 with value: 0.5370529327610872.\n",
      "[I 2025-11-04 10:16:08,533] Trial 28 finished with value: 0.5416309012875536 and parameters: {'criterion': 'log_loss', 'max_depth': 3, 'min_samples_split': 16, 'min_samples_leaf': 1, 'splitter': 'random'}. Best is trial 28 with value: 0.5416309012875536.\n",
      "[I 2025-11-04 10:16:08,567] Trial 29 finished with value: 0.4546494992846924 and parameters: {'criterion': 'entropy', 'max_depth': 12, 'min_samples_split': 10, 'min_samples_leaf': 1, 'splitter': 'random'}. Best is trial 28 with value: 0.5416309012875536.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üå≤ Eng yaxshi parametrlar:\n",
      "{'criterion': 'log_loss', 'max_depth': 3, 'min_samples_split': 16, 'min_samples_leaf': 1, 'splitter': 'random'}\n",
      "‚úÖ Eng yaxshi aniqlik (CV): 0.5416\n",
      "üéØ Test aniqligi: 0.5407\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# üîπ X va y\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# üîπ Train / Test bo‚Äòlish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# üîπ Optuna maqsad funksiyasi\n",
    "def objective(trial):\n",
    "    # Parametrlarni tanlash\n",
    "    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\", \"log_loss\"])\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 2, 30)\n",
    "    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 20)\n",
    "    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
    "    splitter = trial.suggest_categorical(\"splitter\", [\"best\", \"random\"])\n",
    "\n",
    "    # Model yaratish\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        splitter=splitter,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation orqali baholash\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# üîπ Optuna study yaratish\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# üîπ Natijalarni chiqarish\n",
    "print(\"üå≤ Eng yaxshi parametrlar:\")\n",
    "print(study.best_params)\n",
    "print(f\"‚úÖ Eng yaxshi aniqlik (CV): {study.best_value:.4f}\")\n",
    "\n",
    "# üîπ Eng yaxshi modelni testda sinash\n",
    "best_model = DecisionTreeClassifier(**study.best_params, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"üéØ Test aniqligi: {accuracy_score(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9463bc",
   "metadata": {},
   "source": [
    "# Optuna + Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c93db55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-04 10:19:07,329] A new study created in memory with name: no-name-88bf179e-9d0a-4c9d-893b-93a2d2ca5a45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-04 10:19:08,208] Trial 0 finished with value: 0.5402002861230328 and parameters: {'C': 4.28949261590443, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "[I 2025-11-04 10:19:09,011] Trial 1 finished with value: 0.5402002861230328 and parameters: {'C': 3.945542549111091, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:11,672] Trial 2 finished with value: 0.5256080114449213 and parameters: {'C': 8.07897392776962, 'solver': 'saga'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:11,757] Trial 3 finished with value: 0.5350500715307582 and parameters: {'C': 3.583873392923173, 'solver': 'liblinear'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "[I 2025-11-04 10:19:12,432] Trial 4 finished with value: 0.5399141630901287 and parameters: {'C': 0.2323690596996067, 'solver': 'newton-cg'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:15,131] Trial 5 finished with value: 0.5256080114449213 and parameters: {'C': 0.03396279108058619, 'solver': 'saga'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:17,782] Trial 6 finished with value: 0.5256080114449213 and parameters: {'C': 0.046818689276711095, 'solver': 'saga'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:17,870] Trial 7 finished with value: 0.5350500715307582 and parameters: {'C': 1.3602872708706242, 'solver': 'liblinear'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:20,658] Trial 8 finished with value: 0.5256080114449213 and parameters: {'C': 2.1433699177762238, 'solver': 'saga'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:20,739] Trial 9 finished with value: 0.5330472103004292 and parameters: {'C': 0.4381923215609399, 'solver': 'liblinear'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-11-04 10:19:22,885] Trial 10 finished with value: 0.5396280400572245 and parameters: {'C': 0.32300537419251363, 'solver': 'lbfgs'}. Best is trial 0 with value: 0.5402002861230328.\n",
      "[I 2025-11-04 10:19:23,843] Trial 11 finished with value: 0.540200286123033 and parameters: {'C': 8.81401263866425, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:24,740] Trial 12 finished with value: 0.540200286123033 and parameters: {'C': 9.72847358431301, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:25,634] Trial 13 finished with value: 0.540200286123033 and parameters: {'C': 9.850848596078684, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:26,428] Trial 14 finished with value: 0.5298998569384836 and parameters: {'C': 0.01005874401068496, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-11-04 10:19:28,695] Trial 15 finished with value: 0.5399141630901287 and parameters: {'C': 0.7774007428998844, 'solver': 'lbfgs'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:29,501] Trial 16 finished with value: 0.5396280400572246 and parameters: {'C': 1.1970668067323897, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:30,324] Trial 17 finished with value: 0.5390557939914163 and parameters: {'C': 0.11221680989139009, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:31,237] Trial 18 finished with value: 0.540200286123033 and parameters: {'C': 8.717683986276269, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-11-04 10:19:33,430] Trial 19 finished with value: 0.5393419170243204 and parameters: {'C': 2.0031320628510185, 'solver': 'lbfgs'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:34,294] Trial 20 finished with value: 0.5399141630901287 and parameters: {'C': 5.173075180668036, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:35,146] Trial 21 finished with value: 0.5399141630901287 and parameters: {'C': 9.818358573259308, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:36,036] Trial 22 finished with value: 0.5396280400572246 and parameters: {'C': 2.548406617149592, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:36,889] Trial 23 finished with value: 0.5402002861230328 and parameters: {'C': 6.029191626803345, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:37,686] Trial 24 finished with value: 0.540200286123033 and parameters: {'C': 9.886457628902482, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:38,425] Trial 25 finished with value: 0.5396280400572246 and parameters: {'C': 0.7980032344795496, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:39,248] Trial 26 finished with value: 0.5402002861230328 and parameters: {'C': 3.0609539782550312, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n",
      "[I 2025-11-04 10:19:39,343] Trial 27 finished with value: 0.5350500715307582 and parameters: {'C': 5.260992729172911, 'solver': 'liblinear'}. Best is trial 11 with value: 0.540200286123033.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Project_MP\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-11-04 10:19:41,475] Trial 28 finished with value: 0.5393419170243204 and parameters: {'C': 1.584170991526951, 'solver': 'lbfgs'}. Best is trial 11 with value: 0.540200286123033.\n",
      "[I 2025-11-04 10:19:42,303] Trial 29 finished with value: 0.5399141630901287 and parameters: {'C': 4.43799567408094, 'solver': 'newton-cg'}. Best is trial 11 with value: 0.540200286123033.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåü Eng yaxshi parametrlar:\n",
      "{'C': 8.81401263866425, 'solver': 'newton-cg'}\n",
      "‚úÖ Eng yaxshi aniqlik (CV): 0.5402\n",
      "üéØ Test aniqligi: 0.5401\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# üîπ X va y\n",
    "X = df.drop(columns=\"nova_group\")\n",
    "y = df[\"nova_group\"]\n",
    "\n",
    "# üîπ Train / Test bo‚Äòlish\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# üîπ Optuna maqsad funksiyasi\n",
    "def objective(trial):\n",
    "    # Faqat C va solver parametrlarini optimallashtiramiz\n",
    "    C = trial.suggest_float(\"C\", 0.01, 10.0, log=True)\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"lbfgs\", \"liblinear\", \"saga\", \"newton-cg\"])\n",
    "\n",
    "    # Model yaratish\n",
    "    model = LogisticRegression(\n",
    "        C=C,\n",
    "        solver=solver,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Cross-validation orqali baholash\n",
    "    score = cross_val_score(model, X_train, y_train, cv=3, scoring=\"accuracy\").mean()\n",
    "    return score\n",
    "\n",
    "# üîπ Optuna study yaratish\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# üîπ Natijalarni chiqarish\n",
    "print(\"üåü Eng yaxshi parametrlar:\")\n",
    "print(study.best_params)\n",
    "print(f\"‚úÖ Eng yaxshi aniqlik (CV): {study.best_value:.4f}\")\n",
    "\n",
    "# üîπ Eng yaxshi modelni testda sinash\n",
    "best_model = LogisticRegression(**study.best_params, max_iter=1000, random_state=42)\n",
    "best_model.fit(X_train, y_train)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(f\"üéØ Test aniqligi: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3012ca6",
   "metadata": {},
   "source": [
    "| ‚Ññ | Model nomi                       | Eng yaxshi parametrlar                                                                                  | Eng yaxshi aniqlik (CV) | Test aniqligi |\n",
    "|---|---------------------------------|--------------------------------------------------------------------------------------------------------|-------------------------|---------------|\n",
    "| 1 | Random Forest Classifier (Optuna) | {'n_estimators': 234, 'max_depth': 22, 'min_samples_split': 7, 'min_samples_leaf': 3, 'max_features': None} | 0.5597                  | 0.5821        |\n",
    "| 2 | Decision Tree Classifier (Optuna) | {'criterion': 'log_loss', 'max_depth': 3, 'min_samples_split': 16, 'min_samples_leaf': 1, 'splitter': 'random'} | 0.5416                  | 0.5407        |\n",
    "| 3 | Logistic Regression (Optuna)      | {'C': 8.81401263866425, 'solver': 'newton-cg'}                                                       | 0.5402                  | 0.5401        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c8945b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd7ae56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
